{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "Qh9UV-ZI-BEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "C8ePDPrH-FBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "02lAmCwP-4rg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, Dataset, DataLoader, ConcatDataset, Subset\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "6GQfVPo7-GGa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dlS-Yz-7-_u0"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(CNN, self).__init__()\n",
        "      self.layer1 = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                  nn.BatchNorm2d(32))\n",
        "      self.layer2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.BatchNorm2d(64),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "      self.layer3 = nn.Sequential(nn.Linear(2304, 128),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(p = 0.5))\n",
        "      self.fc = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.layer3(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "D1evTCBr-PrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_experiment_datasets(dataset_name = 'CIFAR10'):\n",
        "\n",
        "  if dataset_name == 'CIFAR10':\n",
        "    transform = transforms.Compose([transforms.\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def partition_training_dataset(train_dataset, num_clients):\n",
        "  return torch.utils.data.random_split(train_dataset, [len(train_dataset) // num_clients] * num_clients)\n",
        "\n"
      ],
      "metadata": {
        "id": "eF8Dcqow9nMI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loops"
      ],
      "metadata": {
        "id": "LajPMVm_-IAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T_PYLLZm_MfD"
      },
      "outputs": [],
      "source": [
        "# Training loop on the client side\n",
        "def local_training(client_dataloader, ind, model, optimizer, device, local_training_args = dict(), local_logs = False):\n",
        "\n",
        "    criterion = local_training_args['criterion']\n",
        "\n",
        "    for epoch in range(local_training_args['num_epochs']):\n",
        "      model.train()\n",
        "      loss = 0.0\n",
        "\n",
        "      for i, (images, labels) in enumerate(client_dataloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss += loss.item() #* images.size(0)\n",
        "\n",
        "      epoch_loss = loss / len(client_dataloader)\n",
        "\n",
        "      if local_logs:\n",
        "        print(f\"Client {ind} Training: Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Training loop on the server side + aggregation\n",
        "def federated_training(federated_training_args, local_training_args, server_model, client_data, device):\n",
        "\n",
        "  clients = [[]]*federated_training_args['num_clients']\n",
        "\n",
        "  print('Federated Training!')\n",
        "  for round in tqdm(range(federated_training_args['num_rounds'])):\n",
        "\n",
        "    print(f\"---------- Round {round+1} ----------\")\n",
        "    server_model.train()\n",
        "    server_model.zero_grad()\n",
        "\n",
        "\n",
        "    for client_idx in range(federated_training_args['num_clients']):\n",
        "\n",
        "      client_model = CNN().to(device)\n",
        "      client_model.load_state_dict(server_model.state_dict())\n",
        "\n",
        "      optimizer = torch.optim.Adam(client_model.parameters(), **local_training_args['optimizer_args'])\n",
        "      client_dataloader = DataLoader(client_data[client_idx], batch_size=local_training_args['batch_size'], shuffle=True)\n",
        "\n",
        "      # Train on the client's local dataset\n",
        "      local_training(client_dataloader, client_idx+1, client_model,\n",
        "                      optimizer, device, local_training_args)\n",
        "      clients[client_idx] = client_model\n",
        "\n",
        "\n",
        "    server_model = federated_aggregation(server_model, clients, federated_training_args)\n",
        "\n",
        "  return server_model\n",
        "\n",
        "\n",
        "def federated_aggregation(server_model, clients, federated_training_args):\n",
        "\n",
        "  sd_server = server_model.state_dict()\n",
        "  sd_clients = [cl.state_dict() for cl in clients]\n",
        "  for key in sd_server:\n",
        "    for sd_client in sd_clients:\n",
        "      sd_server[key] += sd_client[key]\n",
        "    sd_server[key] = (sd_server[key] / federated_training_args['num_clients']).float()\n",
        "\n",
        "  server_model.load_state_dict(sd_server)\n",
        "\n",
        "  return server_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluations"
      ],
      "metadata": {
        "id": "CWOoam6D-LZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_acc(test_dataloader, model, criterion, device):\n",
        "  loss_test = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (input, target) in enumerate(test_dataloader):\n",
        "\n",
        "      target = target.to(device)\n",
        "      input = input.to(device)\n",
        "\n",
        "      # compute output\n",
        "      output = server_model(input)\n",
        "      loss = criterion(output, target)\n",
        "      loss_test.append(loss.item())\n",
        "\n",
        "      total += target.size(0)\n",
        "      _, predicted = torch.max(output.data, 1)\n",
        "      correct += (predicted == target).sum().item()\n",
        "\n",
        "    print('Accuracy on the test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "id": "4u_CYJH2-NBb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "HdfHBGx5-TOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BQdnbTRp_PeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6de3406-75e9-4be7-fe9c-250dd7530d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the server model\n",
        "server_model = CNN().to(device)\n",
        "\n",
        "# Define the hyperparameters\n",
        "\n",
        "local_training_args = {'optimizer_args' : {'lr' : 1e-3}, 'num_epochs' : 5, 'batch_size' : 128, 'criterion' : nn.CrossEntropyLoss()}\n",
        "\n",
        "federated_training_args = {'num_rounds' : 4, 'num_clients' : 5}\n",
        "\n",
        "train_dataset, test_dataset = get_experiment_datasets('CIFAR10')\n",
        "client_data = partition_training_dataset(train_dataset, federated_training_args['num_clients'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncG8K9jN_S1y",
        "outputId": "db7160bf-060e-4b50-e1ab-69cd37deb098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federated Training!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Round 1 ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 1/4 [01:11<03:33, 71.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Round 2 ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [02:24<02:24, 72.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Round 3 ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [03:43<01:15, 75.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Round 4 ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [04:57<00:00, 74.34s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "server_model = federated_training(federated_training_args, local_training_args, server_model, client_data, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 128)\n",
        "\n",
        "evaluate_acc(test_dataloader, server_model, local_training_args['criterion'], device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sX-F5TNgh0R",
        "outputId": "fc17e921-cd3e-4f7f-f38f-078f2564d2e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test images: 67.39 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Model"
      ],
      "metadata": {
        "id": "wj7StokK6ufo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "global_data = train_dataset\n",
        "global_dataloader = DataLoader(global_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "global_model = CNN().to(device)\n",
        "optimizer = torch.optim.Adam(global_model.parameters(), lr = lr)\n",
        "\n",
        "print(\"Global Model Training:\")\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  loss_global = 0.0\n",
        "\n",
        "  for i, (input, target) in enumerate(global_dataloader):\n",
        "\n",
        "    target = target.to(device)\n",
        "    input = input.to(device)\n",
        "\n",
        "    # find loss\n",
        "    output = global_model(input)\n",
        "    loss = criterion(output, target)\n",
        "    loss_global += loss.item()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  epoch_loss = loss_global / len(global_dataloader)\n",
        "  # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfNNM4536z8P",
        "outputId": "d60f1861-90b9-4b51-9794-caeca98e714a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Model Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [04:22<00:00, 13.11s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_acc(test_dataloader, global_model, local_training_args['criterion'], device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL86jlqL8xoI",
        "outputId": "b8f9b698-3593-4ea2-88ce-2106162aaaa1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test images: 67.34 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}